# Tutorial LLM

This tutorial will guide you through the process of training a large language model (LLM) step
by step. For educational purposes, we will use a small dataset and a small model, but we will
demonstrate the same basic principles that apply to training larger models on larger datasets.

We provide 2 ways for learning:

- Run the notebook in Google Colab. We ensure all the code could run in Google Colab with free GPU so that everyone can finally train a LLM from scratch. This way is under development, and we will update it soon.
- Run the `run.py` script in your local machine. This code is well-commented and easy to follow. Even if you don't have a GPU, you can still run this code on your CPU by choosing smaller epochs and batch size. This way is under development, and we will update it soon.

## About

The author of this tutorial is Chinese so that we choose a Chinese poem dataset for demonstration. This is good to understand the subtle effect of the model on different training stages for Chinese people. So, I write the notebook in Chinese while keep the code in English.
