# 2.8 Transformer

{% hint style="info" %}
Transformer是现代大语言模型的基石，它是一个精心设计的神经网络结构，由注意力机制及其它网络层组成。从注意力机制，到transformer，再到今天的大语言模型，AI研究者们趟出了一条通往AGI（Artificial General Intelligence，通用人工智能）的道路，曙光依稀可见。
{% endhint %}

上节的最后一个版本引入了可训练参数，从而让注意力权重有了更多可能性。但请注意，图2.6中间行所代表的计算与前一个版本并无区别，都是矩阵乘以它自身的转置，即

$$
W = X^{\prime} X^{\prime T}
$$

只不过输入矩阵从X变成了X'。这种计算方式得出的结果有一个术语，叫“自相关矩阵”，因为它记录了矩阵X与其自身的相关性。回顾图2.6的中间行，可以看出是X的任意两个行向量之间的相关性。

这种自相关有一个致命的缺陷——它是对称的。从图2.6中间行的公式就可以看出，计算得出的W是个对称矩阵。原因也很容易理解，向量i与向量j的相似性等同于向量j与向量i的相似性，余弦相似度不区分谁在前谁在后。但结合我们的实际场景，字与字之间的注意力需要这个对称性吗？

答案是不需要。请读者不要忘了，语言是有顺序的。一个词正着说和倒着说往往会表达截然不同的意思。一个词对出现在它后面的另一个词很重要，不代表后面那个词对这个词也同样重要。

打破对称性方法不止一种。

第一种方法是让上面公式中的左右两个矩阵不同。既然X'是通过一个参数矩阵M变换得到的，那干脆再拿另一个参数矩阵N变换得到Y'，然后让X'和Y'相乘得到W，以此打破对称性。计算过程如下式所示。

$$
X^{\prime} = MX \\
Y^{\prime} = NY \\
W = X^{\prime} Y^{\prime T}
$$

第二种方法则更为深邃。既然W的左下角和右上角对称，不如我们只用左下角，右上角直接扔掉。这是个非常有洞见的做法，不仅仅去掉了对称性，更是会大幅提高模型的因果推断能力。这又是怎么一回事？

重新审视W每个元素的含义，我们会发现，第i行第j列的w元素代表叠加多少权重的向量j到向量i上。比如，W\[3,1]表示向量X\[1]对向量X\[3]的影响力。在后续操作中，i和j指向的向量扮演着不同角色。行数i指向的是当前向量，是我们打算更新的向量，也就是X\[3]，而列数指向的则是参考向量，即X\[1]。此时，我们注意到，W的左下角元素有一个共同特征，它们的行数都大于列数。这一现象其实有一个颇具现实意义的解释。当前向量的下标总是大于参考向量的下标，意味着当我们搜寻注意力的时候，我们只能往左看，而不能往右看。有没有体会到其中的道理？人说话其实就是如此，我们只能参考已经说出的话来决定下一个字，而无法参考未来将要说出的话，否则就违反了因果性。

虽然我们从W的对称性引出了这两种方法，但事实上它们的作用比我们想象的更大。

在实际的注意力机制中，除了使用M和N对输入变换得到两个版本的输入外，还额外增加了第三个版本，用于最后与权重矩阵W相乘。为了清晰说明这三个矩阵的作用，研究者们给它们起了响亮的名字，分别是Query、Key和Value。Query矩阵对应于我们这里的M，Key对应于我们的N。这三个名字直观地表达了它们各自代表的含义。经过Query变换后的向量在计算W的时候作为左操作数，代表了正在搜寻注意力的当前向量，因此使用“Query（查询）”一词。同理，经过Key变换后的向量在计算W的时候则作为右操作数，代表了被检视的参考向量，只有关键（key）信息才会被当前向量注意到。经过Value变换后的向量则是原始输入的一个化身，权重矩阵W将施加在该向量上以得到最终结果。这三个参数矩阵使得模型可以学到更多更复杂的知识，并有能力拆分注意力的各个方面，可以说相当精妙。

而方法二增加的因果限制则彻底让注意力机制成为主宰语言模型的关键。试着想象，如果任何一个字都既可以参考前面的字又可以参考后面的字，它的向量就包含了许多来自未来的信息。然而实际使用时与训练场景不同的是，我们希望模型一个字一个字地输出。此时，最后一个字的向量只能包含所有出现在前面的信息，但这些信息有可能不足以支持它做出正确的预测。也就是说，本质上我们的语言就是包含因果性的，所以如果在训练时不限制因果性，模型就有可能在使用时表现不佳。

至此，注意力机制的大部分内容都已讲解完毕，我们来看一看完整的结构长什么样。





{% hint style="info" %}
有趣的是，transformer始终没有一个合适的中文翻译。“变换器”、“变压器”感觉都不是太对味。因此，文中会直接使用这个单词的英文原文。transformer是动词transform（变换）的名词形式，可以理解为一种可以对数据做变换的装置。
{% endhint %}

