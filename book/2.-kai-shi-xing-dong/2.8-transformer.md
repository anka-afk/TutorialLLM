# 2.8 Transformer

{% hint style="info" %}
Transformer是现代大语言模型的基石，它是一个精心设计的神经网络结构，由注意力机制及其它网络层组成。从注意力机制，到transformer，再到今天的大语言模型，AI研究者们趟出了一条通往AGI（Artificial General Intelligence，通用人工智能）的道路，曙光依稀可见。
{% endhint %}

上节的最后一个版本引入了可训练参数，从而让注意力权重有了更多可能性。但请注意，图2.6中间行所代表的计算与前一个版本并无区别，都是矩阵乘以它自身的转置，即

$$
W = X^{\prime} X^{\prime T}
$$

只不过输入矩阵从X变成了X'。这种计算方式得出的结果有一个术语，叫“自相关矩阵”，因为它记录了矩阵X与其自身的相关性。回顾图2.6的中间行，可以看出是X的任意两个行向量之间的相关性。

这种自相关有一个致命的缺陷——它是对称的。从图2.6中间行的公式就可以看出，计算得出的W是个对称矩阵。原因也很容易理解，向量i与向量j的相似性等同于向量j与向量i的相似性，余弦相似度不区分谁在前谁在后。但结合我们的实际场景，字与字之间的注意力需要这个对称性吗？

答案是不需要。请读者不要忘了，语言是有顺序的。一个词正着说和倒着说往往会表达截然不同的意思。一个词对出现在它后面的另一个词很重要，不代表后面那个词对这个词也同样重要。

打破对称性的方法不止一种。

第一种方法是不要用X'乘以自己。既然X'是通过一个参数矩阵M变换得到的，那干脆再拿另一个参数矩阵M2变换得到X2'，然后让X'和X2'相乘得到W，以此打破对称性。计算过程如下式所示。

$$
X^{\prime} = MX \\
X_2^{\prime} = M_2Y \\
W = X^{\prime} X_2^{\prime T}
$$

第二种方法则更为深邃。既然W的左下角和右上角对称，不如我们只用左下角，右上角直接扔掉。这个方法初看有点邪乎，但其实充满洞见。它不仅可以抹除对称性，更是会大幅提高模型的因果推断能力。不过，这又是怎么一回事？

重新审视W每个元素的含义，我们会发现，第i行第j列的w元素代表叠加多少权重的向量j到向量i上。比如，W\[3,1]表示向量X\[1]对向量X\[3]的影响力。在后续操作中，i和j指向的向量扮演着不同角色。行数i指向的是当前向量，是我们打算更新的向量，也就是X\[3]，而列数指向的则是参考向量，即X\[1]。此时，我们注意到，W的左下角元素有一个共同特征，它们的行数都大于列数。这一现象其实有一个颇具现实意义的解释。当前向量的下标总是大于参考向量的下标，意味着当我们搜寻注意力的时候，我们只能往左看，而不能往右看。有没有体会到其中的道理？人说话其实就是如此，我们只能参考已经说出的话来决定下一个字，而无法参考未来将要说出的话，否则就违反了因果性。

虽然我们从W的对称性引出了这两种方法，但事实上它们的作用比想象的更大。

在实际的注意力机制中，除了使用M和M2处理输入得到X'和X2'外，还额外增加了第三个变换矩阵，用于最后与权重矩阵W相乘。为了清晰说明这三个矩阵的作用，研究者们给它们起了响亮的名字，分别是Query、Key和Value。Query矩阵对应于我们这里的M，Key对应于我们的M2。这三个名字直观地表达了它们各自代表的含义。经过Query变换后的向量在计算W的时候作为左操作数，代表了正在搜寻注意力的当前向量，因此使用“Query（查询）”一词。同理，经过Key变换后的向量在计算W的时候则作为右操作数，代表了被检视的参考向量，只有关键（key）信息才会被当前向量注意到。经过Value变换后的向量则是原始输入的一个化身，权重矩阵W将施加在该向量上以得到最终结果。这三个参数矩阵使得模型可以学到更多更复杂的知识，并有能力拆分注意力的各个方面，可以说相当精妙。

而方法二增加的因果限制则彻底让注意力机制成为主宰语言模型的关键。试着想象，如果任何一个字都既可以参考前面的字又可以参考后面的字，它的向量就包含了许多来自未来的信息。然而实际应用与训练场景不同的是，我们希望模型一个字一个字地输出。此时，待预测的字只能包含所有出现在前面的信息，但这些信息有可能不足以支持它做出正确的预测。也就是说，本质上我们的语言就是包含因果性的，所以如果在训练时不限制因果性，模型就有可能在使用时表现不佳。

至此，注意力机制的大部分内容都已讲解完毕。把这两节介绍的所有内容放在一起，就形成了所谓的transformer网络结构。我们对照下图，看一看宏观上tranformer到底在做什么。

<figure><img src="../.gitbook/assets/transformer.png" alt=""><figcaption><p>图2.7 transformer网络结构</p></figcaption></figure>

从左上角开始，输入的255个字表示为255个1024维向量。这些向量经过Query、Key和Value矩阵后扩展成三份，但各自仍然是255个1024维向量，只不过代表了不同的含义。经过Query变换后的输入向量关注当前字的信息，并寻求与其它字的相关性。经过Key变换后的输入向量关注该字作为参考字时的信息，并寻求与当前字的相关性。经过Value变换后的输入则包含了更完整的当前字的信息，并准备在各个字之间叠加注意力。Query和Key处理后的向量会先计算相似度，得到右上角所示的权重矩阵，然后将矩阵的右上元素全部置零，以维持因果性。

接下来是一个前文未提到的操作，对权重矩阵施加softmax函数。效果是让每一行的数加起来和为1，相当于把原始的相似性大小转变为一个概率。在概率视角下，我们就能确定叠加多少比例的参考字向量到当前字向量，于是注意力机制有了清晰的物理含义。

Value分支与softmax之后的权重矩阵相乘，就得到结合了注意力的一组字向量。这组向量的维度和输入完全一致，可以认为原始的每个字向量摇身一变，虽然身份还是代表那个字，但内在增添了许多对周围其它字的理解。

再往下，则是一串典型的传统神经网络结构，被称为前馈神经网络（Feedforward Neural Network, FFN）。先经过一个1024×4096的矩阵变换，将每个1024维字向量转换为4096维字向量。然后经过一个ReLU函数，这个函数非常简单，当输入大于0时原样输出，小于等于0时直接输出0，相当于对负信号的截断。ReLU看起来简单，但没它还真不行，它引入的非线性是神经网络超强拟合能力的关键。最后，使用一个4096×1024的矩阵将每个4096维字向量再变回1024维。没错，最后的输出仍然是255个1024维向量，好像实现了一次轮回。

整个过程中，注意力计算是大模型成功的关键。正是注意力机制让AI可以有选择地结合上下文，使每个字的内在表示变得更丰富。但我们也不能忽略后面的前馈神经网络，特别是其中先升维再降维的操作。与注意力机制不同的是，前馈神经网络中的两个线性变换针对每个字向量单独操作。换句话说，只用一套参数处理所有字向量。我们可以把注意力机制和这里的线性变换层看作互为正交的两个步骤。注意力机制注重字和字之间的关系，但并不十分在意字向量本身含义的变化。诚然，字向量在注意力机制前后肯定是变化的，但变化主要体现在对其它字信息的融合，而不是它自身含义的变化。前馈网络则恰恰相反，由于只关注每个字向量本身，模型可以学会如何更好地在高维空间中表达字的含义。升维操作就好像让模型用白话文解释文言文，它自然能解释得更细致，同时获得更准确的理解。ReLU在它的白话文解释上切除那些负值，正如告诉模型你的解释里存在错误，被我好心修复了。而最后的降维操作则是再次把白话文解释压缩为简略的文言文，且含义在维度变化前后保持不变。这既能体现模型对文本含义的深刻理解，也利于降低计算量。

我们把以上的模型结构称为transformer，寓意对文本含义的“变换”。

{% hint style="info" %}
有趣的是，transformer始终没有一个合适的中文翻译。“变换器”、“变压器”感觉都不是太对味。因此，文中会直接使用这个单词的英文原文。transformer是动词transform（变换）的名词形式，可以理解为一种可以对数据做变换的装置。
{% endhint %}

不过，实际大模型中的transformer和上图相比仍有一些差别，但不至于影响整体结构。我们简单提一下， 不必关注细节。

* 一套Query、Key和Value通常被称为一个注意力头（attention head）。为了增强模型的能力，实际应用中，人们一般会创建多个注意力头，并在注意力机制的末尾合并每个注意力头生成的字向量。
* Query、Key和Value矩阵的列数不一定非要和输入向量一致。只需要保证注意力机制最终输出的字向量仍然与最初输入的字向量维度一致即可，各个中间结果的维度其实都可以自行调整。
* 注意力头和前馈网络建议采用残差连接。它的意思是，注意力机制最后输出的结果最好只是初始输出应该变化的量，这样就可以把输出和输入加起来作为完整的结果。这种做法始于经典的ResNet，人们发现学习残差比学习完整的向量更容易，因为改动只是原始信息的一小部分，记起来没那么困难。

需要提醒大家的是，transformer只是一个架构，不是完整的模型。下一节，我们会介绍如何使用transformer搭建完整的模型，并正式开始训练。
