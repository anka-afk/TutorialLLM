# 2.2 从数据出发

正如前文提到的，我们通过规定输入输出的映射来规定AI的行为，因此，如何收集数据，如何设计合理的输入输出就是第一件要考虑的事情。

出于计算资源的考虑，我们把任务目标局限到古诗这个语境内。原因有二，一是因为中国的古诗本身就不多，质量高且数据量不大。二是容易看到训练效果，同时人们一般不会对生成的古诗有太高期待。

在配套代码仓库中，我已经事先准备好了中国的所有古诗，放在[data.json](../../data.json)文件里，以JSON格式保存。整个JSON对象是一个数组，数组中的每个元素是一首诗。我把开头第一首诗的部分贴在下面。

{% code overflow="wrap" %}
```json
[{"author": "太宗皇帝", "paragraphs": ["秦川雄帝宅，函谷壯皇居。", "綺殿千尋起，離宮百雉餘。", "連甍遙接漢，飛觀迥凌虛。", "雲日隱層闕，風煙出綺疎。"], "title": "帝京篇十首 一", "id": "3ad6d468-7ff1-4a7b-8b24-a27d70d00ed4"}, ...]
```
{% endcode %}

可以看到，每首诗包含作者、段落、标题、ID等字段。

按照之前的计划，我们只需要根据这些数据生成合适的输入输出就行了。不过，真正开始实现，我们就会发现一个之前没有考虑到的问题。

输出是输入文本的下一个字，这很好理解。但输入文本到底取多长呢？如果默认把整个一条数据当作输入，比如，我们把这首诗写成如下的形式：

```
Input:
帝京篇十首 一
秦川雄帝宅，函谷壯皇居。
綺殿千尋起，離宮百雉餘。
連甍遙接漢，飛觀迥凌虛。
雲日隱層闕，風煙出綺疎
Output:
。
```

是不是有点莫名其妙？输入一大堆，结果让模型预测最后的句号？这样的话模型肯定什么也学不会。

所以我们不能只让模型预测最后面的字，而是每个字它都要学会预测。但每个字所处的位置不同，我们只能用前面的文本预测后面的字，而不能用后面的文本预测前面的字，毕竟我们不可能倒着说话。

这就意味着，我们应该把这一条数据拆分成多条。每条预测一个不同位置的字。比如，从这首诗拆分出的第一条数据为

```
Input:
帝
Output:
京
```

没错，输入只有一个字。因为我们希望预测第二个字“京”，而能够用来预测“京”的就只有“帝”这个字了。以此类推，拆分出的第二条数据为

```
Input:
帝京
Output:
篇
```

有没有找到规律呢？假设这首诗总共N个字，按照这种拆分方法，我们最后会得到N-1条数据。除了不预测第一个字，其它所有字都要预测。

模型由此学会从任何一个位置开始往后接话，而不是从某个特定的位置。因为我们最终的目标是让模型能够独立生成一首完整的诗，它必须有能力从开头开始输出。当它输出一个字后，这个字可以加入输入文本，继续生成下一个字，直到整首诗全部生成。

不过，此时，我们又会面临另一个问题。按照这样的输入输出规则，模型岂不是必须从头阅读一首诗？在我们生成的N-1条数据中，输入全部都从这首诗的标题开头，然后是作者、正文。如果我想要直接给它输入一个上句，让它输出下句，估计它又会傻眼了。

为了解决这个问题，我们或许可以把起始点变一变。刚才，我们每次都从第0个字开头，现在，我们可以从0 \~ N-1里面任选一个位置开头。不过，这样处理起来会有些麻烦，因为不光开头要变，结尾也要跟着变，可选的组合会非常多。

一个更好的方案是，把所有诗从头到尾连起来组成一个超级长的文本。然后，随机取一个位置作为起始点，将输入的最大长度固定为N。像刚才一样，分别取长度为1, 2, ..., N-1的文本作为输入，得到N-1条数据。这种方法生成的输入输出变化最多，起始位置、长度都不确定，模型因此可以学到更多样化的知识。

在我们的实现中，取N=256，随机选取起始位置后得到一个长度为256的文本，如下所示。

```
臨高城。
嗟我久離別，羨君看弟兄。
歸心更難道，回首一傷情。

金陵夜泊
冷煙輕澹傍衰叢，此夕秦淮駐斷蓬。
棲鴈遠驚沽酒火，亂鵶高避落帆風。
地銷王氣波聲急，山帶秋陰樹影空。
六代精靈人不見，思量應在月明中。

題天申宮三首 二
中峰嘉樹綠陰陰，洞裏靈蹤已遍尋。
欲下山門又迴首，數聲清磬白雲深。

賦得魚登龍門
魚貫終何益，龍門在苦登。
有成當作雨，無用恥爲鵬。
激浪誠難泝，雄心亦自憑。
風雲潛會合，鬐鬣忽騰凌。
泥滓辭河濁，煙霄見海澄。
迴瞻順流輩，誰敢望同升。

寄京城親友二首 一
苦吟看墜葉，寥落共天
```

这里面包含了5首诗，其中第一首和最后一首不完整。按照前面的规则，从这256个字的文本可以进一步生成255条数据，每一条的输出对应于从第2个字开始的每个字。想必读者已经心中有数，就不再详细列出了。

唯一仍可能让人感到疑惑的是那些不完整的诗句。的确，由于设定了最大长度，我们截断了一些诗句。但这并没有什么问题。在后面的章节中，我们会一步步了解到模型训练的三个阶段——预训练、微调和对齐。在第一步预训练阶段，我们只需要让模型尽量多地掌握语言的底层逻辑，而不需要关心其输出的长度或格式。事实上，预训练模型的目的是让模型根据给定的输入输出下一个字，并且反复输出下去，直到规定的最大长度。此时的模型并没有完整的段落概念，它并不能用来对话（Chat），而只能用来补全（Completion）。因此，你会看到在我们这段文本中，一首诗结束了又接着下一首，模型会学会一首接一首地写诗，直到字数限制。
