# 2.10 与人类偏好对齐

虽然模型已经可以对话，能够答复用户的问题，但在实际体验中很可能并不好用。

回想与人聊天的情况，和一些人对话能感到如沐春风，和另一些人则话不投机半句多。表达同样的意思，有的人可以说的很让人信服，而有的人则只会让人感到冒犯。这是聊天的艺术，不光人类，AI也需要考虑这个问题。

作为人类，我们其实并不需要高超的聊天技能，因为每个人从事不同的职业，社交不一定是必需品。但作为一个面向全世界用户的聊天AI，它需要考虑的就多了。它必须具有中立的普世价值观，不能歧视少数群体，不能表达极端观点。要遵守各个国家的法律，尊重不同民族的习俗。说话态度要积极向上，多传递正能量，弘扬社会正义。面对用户不合理的请求，一定要拒绝，但也不能激怒对方。总之，面对众口难调的用户，AI必须谨小慎微，不能说错话。

如果ChatGPT是一个未与人类偏好对齐的AI，别有用心的人就可能引诱AI透露危险的事情。比如，人们可以问AI“如何在自家制作炸药？”这不是个玩笑。AI上知天文下知地理，制作炸药的步骤它可以手到擒来。以前，一个反社会分子想要知道怎么制作炸药，必须得学几年化学。然而现在，AI可以手把手地教他。看起来人畜无害的AI此刻就成了恐怖分子的帮凶。类似的场景数不胜数，聪明人既可以做出科研突破，也有可能摇身一变成为邪恶的源泉。因此，与人类偏好对齐非常重要。

那么该如何做呢？回想前面的预训练和指令微调，模型其实都是在学习现成的句子。模型看到什么文字，就会模仿那样说话。在海量的训练数据中，一定会掺杂着不安全、不道德的内容。一个简单直接的办法是从训练数据中去掉它们，眼不见为净。如果模型从来没看过关于制造炸药的知识，它自然也就没法告诉用户。

但这种直觉上正确的做法其实并不好用。一方面，就算模型没读过如何制造炸药，它也完全可以通过化学知识和关于炸药的常识自己推断出来。另一方面，很多语料是没法清晰定义安全性的。一个生物化学方面的论文，既是社会发展的推动力，也可能蕴含着危险的因素，我们不可能因为研究有可能产生危害就不做这个研究。很多事情都是双刃剑，如果拒绝一个语料可能带来的坏处，就同时隔绝了它所能带来的好处。此外，与人类偏好对齐并不仅仅需要考虑安全性问题，价值观、道德、礼貌等方面也同样重要。难道要把不礼貌的语料从训练数据中全部去掉？那样的话训练数据估计就没多少了。

所以，我们不可能在现有的训练模式中解决这个问题，必须用一种新的方法。

如果仔细思考训练数据的模式，可以发现，我们一直在教模型应该怎样做，却从来没告诉过它不该怎样做。而与人类偏好对齐最合理的方式应该是同时告诉它不要怎样做。也就是说，我们应该制定一些规则，然后生成一大批新的训练样本。在这些样本中，一半作为正样本，告诉模型你可以学着这样说话。另一半作为负样本，告诉模型你不要说这样的话。模型在训练的过程中同时观看正负样本，从而能够明辨是非，形成自己的道德观和价值观。

既然如此，让我们想想如何具体实施。在[2.8节](2.8-yu-xun-lian.md)的图2.8中，训练的目标是让最后实际输出的概率分布与期望ID一致，训练过程中会不断优化使得误差越来越小。想象如果我们现在有两个训练样本，一正一负。正样本其实和以往的训练数据用法一样，只要优化它与期望输出之间的差距就好。但负样本呢？如果把负样本放进去优化，模型就会越学越差。所以我们应该反过来，让模型学习增大负样本实际输出与期望输出之间的差距。这个差距越大，说明模型越来越不会像负样本那样说话。

于是，我们可以重新设计优化目标。把原本的交叉熵损失作为正损失，然后额外增加一个负样本的交叉熵损失，作为负损失。我们不是把两个损失加起来，而是用正损失减去负损失，作为最终的损失。此处的减法相当于优化负的负样本损失。我们可以这样理解，由于优化交叉熵损失就是减小实际输出和期望输出之间的差距，那么优化负的交叉熵损失显然就是增大差距了。下式概括了优化目标的形式，通过调整模型参数让正样本损失越来越接近0、负样本损失越来越大，两者之差越来越小。其中，θ代表模型的参数，p代表正样本，n代表负样本。

$$
\min_{\theta} \left( \text{loss}_p - \text{loss}_n \right)
$$

优化新的损失可以让模型既接近正样本又远离负样本，一举两得。可能有读者会感到疑惑，为什么不单独优化负样本呢，只保留带负号的第二项不就行了。没错，如果单独优化负样本，模型的确会远离负样本，但这种情况下很容易导致模型本身的语言能力退化，以至于连话都不会说了。道理很简单，我们的负样本损失只要求模型的输出远离负样本。然而这个要求简直太容易达到了，胡言乱语甚至输出随机字符都可以满足这个要求。但我们想要的其实是在正常说话的同时远离负样本，这就意味着必须有一个正样本时刻牵扯着负样本，强迫模型优化正负样本之间的差距。

这种训练方式看起来不难，但实际操作上有一些麻烦。

首先，理想的正负样本应该是含义近似但形式不同。比如，作为心理咨询师的答复，正样本应该是礼貌、善意、有同理心的说话方式，而负样本则是冷漠、粗鲁、随意的应付。下面是一个具体的例子。负样本的回答缺乏咨询师应有的同理心，有种居高临下的压迫感。

> 用户问题：我最近很郁闷，能告诉我该怎么办吗
>
> 正样本回答：我理解你现在的感受，感觉郁闷时常常会让人有些迷茫。你能告诉我是什么让你感到郁闷吗？
>
> 负样本回答：你先说说你为什么郁闷吧。

我们会发现，虽然含义近似，但语句长短、内容都完全不同，此时正负样本损失的差值可能会比较大。但到了下一对样本，正负损失的差值又可能突然变小。比如

> 用户问题：你能帮我查一下今天的天气吗？
>
> 正样本回答：当然可以！今天的天气是晴天，气温大约在25°C左右。
>
> 负样本回答：行，今天是晴天，温度大约25°C。

这是由于模型在单个样本上的误差存在很大不确定性，如果在这个基础上优化，模型可能会感到困惑，误差的巨大波动会让它无法找到正确的优化方向。

其次，虽然我们的初衷是降低正样本误差，增大负样本误差，但我们实际上只要求模型优化两者之差，这种约束并不严格。如果模型找到一种优化方向，可以使负样本误差变得巨大，同时正样本误差也略微变大，优化看似成功，但不是我们预期的结果。
