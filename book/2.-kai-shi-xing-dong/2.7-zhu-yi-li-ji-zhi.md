# 2.7 注意力机制

图2.3的连线其实有更为直观的解释。由于每组向量对应于一个字，我们可以认为它构建了字与字之间的连接关系。细心的同学可能发现，虽然中间层每组向量的元素个数可以与输入不同，但向量的组数却仍然是255个。图中可以看到，每个输入向量都发出了一条水平箭头指向右侧的中间层向量。这其实是经过精心设计的。

理论上，中间层的组数也可以任意设置，就像图2.2中的连接一样，不用考虑中间层具体有多少个神经元。但让它和输入层数量一致有明显的好处。一方面，从语义上我们更容易区分每个中间层向量到底与哪个字对应；另一方面，我们可以增加层数，在中间层和输出层之间再插入若干个中间层，而且每一层的每个向量都可以对应于特定的字。

当向量与字对应之后，我们就对整个神经网络在做的事情有了一个宏观的了解。比如，我们可以认为，图2.3中间层的第一组神经元尝试从输入的255个字中提取信息，以加深自己对第一个字的理解。换句话说，第一个字在1024维空间中的高维表示从输入向量变成了中间层的输出变量，而且综合了其它字的信息。你可能暂时想不明白这个这个变化意味着什么，我们来看个例子。

假设输入的是这么一句话：“只要你向着阳光走，影子就会躲在后面”。这句话里面，“光”代表光明的未来，是一种积极的意象。但如果换成这句：“我炒股把家底都输光了”。这里面的“光”则表示空空如也，暗示着悲惨的结局。多义字在中文并不少见，这也是为什么2.5节强调了我们要在高维空间中表示字的含义。然而，虽然输入层的“光”需要包括它所蕴含的各种含义，中间层则不一定。在看过了整句话中的其它字后，作为人类，我们很容易明白，“光”在当前这句话中具体体现的含义。于是，中间层的“光”可以只保留有关的含义而丢弃无关的含义，向量在1024维空间中从原始位置移动到另一个更接近其真实用意的位置。这便是我们希望神经网络做的事情。

{% hint style="info" %}
我们是在从神经网络的结构推测其内部的工作机制，虽然不一定对，但结构与功能通常是紧密相关的。正因为我们希望它以某种方式处理数据，我们才把网络设计成特定的样子。本文有时以目标出发引导大家想出合理的方案，但有时我会直接给出合理的方案，然后解释为什么这样设计。希望读者可以跟随文中的思路，必要时停下来思考一下再往后读。
{% endhint %}

如果我们再进一步细化这件事，
