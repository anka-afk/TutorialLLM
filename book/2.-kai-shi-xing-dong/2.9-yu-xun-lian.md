# 2.9 预训练

细心的话，你或许会发现图2.7的输入和输出维度完全一致，这是精心设计的结果。transformer就像一块积木，同样规格的输入输出使得人们可以把它一层一层搭在一起，从而提升整个模型的能力。

在AI领域有一个神奇的规模定律（Scaling Law）。越多的参数、算力和数据，就能训练出越强的模型。大模型之所以取得突破，正是因为算力上来了。当模型变大，效果自然展现。

虽然本文使用一个教学性质的小模型，但多层transformer仍然是必须的。我们把图2.7的内容用一个transformer block代替，在下图中展示完整的模型结构和训练过程。

<figure><img src="../.gitbook/assets/tutorial_llm.png" alt=""><figcaption><p>图2.8 TutorialLLM的网络结构</p></figcaption></figure>

输入数据是2.3节展示过的一段诗句——“白日依山盡，黃河入海”，对应的预期输出是“流”，图中直接用ID表示文字。输入的ID先经过一个Embedding Table，查表找出每个ID对应的64维向量。注意，前面讲解时我们一直说字向量维度是1024。但受计算资源限制，要想在自己电脑上跑起来，只好把维度降低到64。Embedding Table的输出是10个64维向量，对应输入的10个字。

这组向量经过4个Transformer Block，输出仍然是10个64维向量。中间的过程想必读者已经了解，仍不清楚的同学可以回到前几节温习一下。

接下来，用一个Unembedding Matrix将每个64维向量变成8548维向量，这是Embedding Table的逆过程。事实上，对Embedding Table的查表操作可以等价于一次矩阵乘法。举例来说，对于第一个ID 4403，要想取出Embedding Table的第4403列向量，可以通过下面的矩阵运算来实现。

$$
\begin{bmatrix}
c_0 & c_1 & \dots & c_{8547}
\end{bmatrix}
 \begin{bmatrix}
0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 0
\end{bmatrix}
\rightarrow c_{4403}
$$

其中，ci表示Embedding Table的第i个列向量。我们只需要构造一个8548维的One-Hot向量，它的第4403个数是1，其余都是0。这样运算的结果就恰好是Embedding Table的第4403个列向量。

反过来，如果把Unembedding Matrix当作Embedding Table的逆矩阵，用它左乘64维向量，就可以得到一个8548维向量。

最后，该怎么衡量实际输出与期望输出之间的差距呢？一边是实际输出的8548维向量，另一边是期望的ID 3486。虽然格式不一样，但办法很简单。我们只需要把实际输出的8548维向量的每个数当作每个字的概率，然后只看第3486个概率就行了。正确的那个字概率越大，离期望输出就越近。至于其它字之间的相对大小，其实并不重要。这种方法其实就是交叉熵损失（Cross Entropy Loss），在多分类问题中非常常见。我们这个案例本质上也是一个多分类问题，从8548种类别中选出正确的那个。

交叉熵损失的结果会被用来计算梯度，并反向传播通知模型的各个参数。在整个神经网络中，每个可训练参数都需要对最后的误差负责。根据整个网络的连接关系和各个参数的权重，反向传播算法可以计算出各个参数在其中起到的作用，从而微调它们的值。

以上，我们看到了一条训练数据从输入到输出的全过程。在实际训练时，我们需要准备大量数据，2.2节曾提到过，将所有文本首尾相连，然后随机选择起点，分别取长度为1, 2, ..., N-1的文本作为输入，得到N-1条数据。当时没有提到的是，Transformer架构对处理这样一组数据有天然的优势。虽然图2.8展示了如何处理一条数据，但实际上，这N-1条数据可以并行处理。我们只输入长度为N-1的最后一条数据，在网络的末尾，与图中只选取最后一个字向量不同，我们用Unembedding Matrix对所有字向量做变换，得到N-1个8548维向量，每个向量都对应于一个字。同时，我们也可以找出每个字对应的下一个字作为期望输出，从而一次性构造N-1对实际输出和期望输出。这N-1对输出都可以用来计算交叉熵损失，将所有损失加起来交给优化器统一进行反向传播。

不过，你可能会奇怪，为什么每个字向量都可以用作对下一个字的预测呢？这个问题最开始也困扰了我很久，但后来发现，这正是因果性带来的额外好处。要想通这个问题，可以追踪一下最后一层某个中间位置的字向量到底是怎么算出来的。比如，假设N是256，那么完整的输入是255个字。如果我们盯着第10个字看，会发现它在注意力机制中只用到了前9个字的信息，因此就算我们把第10个字后面的所有字去掉，它的字向量并不会发生任何变化。这说明注意力机制对它没有影响。那前馈神经网络呢？上一节我们其实提到过，前馈网络只关注每个字向量自身，它连前面的9个字都不看，更别提后面的字了。所以，在整个网络中，从输入到输出，第10个字的向量只依赖于前9个字。于是，最后一层的每个字向量都可以用来预测下一个字，实现并行的效果。

虽然本文不打算详细讲解代码实现，但我们可以看一下代码运行的效果，好有个直观的认识。

{% hint style="info" %}
本文提到的所有代码都可以在GitHub仓库[TutorialLLM](https://github.com/jingedawang/TutorialLLM)中访问。想要亲自实践的朋友，可以参考README中的使用方法。
{% endhint %}

训练过程中，每迭代100次，输出模型在训练集和测试集上的损失，并让模型以“春夜喜雨”开头写诗，写满256个字。我们预期损失值在两个数据集上同步下降，且写的诗质量越来越高。以下是部分输出片段。

```
Step 0, train loss 0.0000, evaluate loss 9.2029
Generate first 100 characters of poems starting with 春夜喜雨:
春夜喜雨墻鵷函媵分塏潏鍳母付菱莽换驃慚憮儲躕袗鯆溫沲芳罔窻倏菂弓匌莿尚茸茇培嵍鵝掣卵耽敧青魄叚𪆟瞑唱鄢懅齧泉綘躅鷂㦬烻超玃鯽敝俱惏廏鐐處翻矼奭媼悟出撾孃詠可碙媌鶂旐垤嵼鶤柘輩噇篲詮擲憇純絃蜘儔緬簇澎雨搰褚磐歙
Step 100, train loss 6.3026, evaluate loss 6.2829
Generate first 100 characters of poems starting with 春夜喜雨:
春夜喜雨剡冰 閒路。耕銅崔出玫胖，上惺尋。


樹天歸贈，。


強寥下戎景人。
火亂潔頴明戍流漸道一室勢。
摶磻四春相似
，年陽下了行爭癥追赤且木少塞是門相。
摘言迎來夢賦示營似吾首。
暮墜鬢纔征蕉春崖五牀

<Omit many iterations here...>

Step 5000, train loss 4.7134, evaluate loss 4.9221
Generate first 100 characters of poems starting with 春夜喜雨:
春夜喜雨發早，唯應尋與塵遠山。
尋敬況皆迷情重，見買腰量漢寒宵。

贈家石橋危師兼寄一題因作四使者池應制
梧桐葉朝廷塵，一曲鐘明冠黑。
西上國邀辭外，主詞高須禁圖違。
因循勢祿應承急，不知育符惟開。
舊玄元侍

<Omit many iterations here...>

Step 10000, train loss 4.3883, evaluate loss 4.6798
Generate first 100 characters of poems starting with 春夜喜雨:
春夜喜雨人掃轉，煙穗半開笑倚簷。
美酒飄生弱氈粉，記詩繫舊光桑榆。
鶯別夜悲行滿袖，簟前年態踏爲顏。
雲間葉絡長簾鉢，舊漏孤燈照水香。
尊容暮事黃雲伴，使我清光小舊狂。

重夜涼
自謂長安不速時，當來不得鷓鴣
```

当step=0时，模型还没开始训练，它的参数全部来自随机初始化，因此输出也是完全随机的数，看起来就像乱码一样。

当step=100时，模型开始尝试使用标点符号并换行，但用的乱七八糟，文字也没有任何含义。

当step=5000时，格式上终于有了起色，模型正在努力让每一行输出两句话，但每句话的长短仍不尽如人意。不过，我们可以发现，它的用词变得合理了许多，虽然仍然没有组成完整的句子。

当step=10000时，格式变得非常整洁，外观看起来像一首七言诗。细看每句话，能感到一丝韵味，仿佛学到了一点诗的精髓。但客观来说，它对语句的整体把握还是差了些，一方面押韵不成功，另一方面缺乏主旨，略显空洞。

虽然不完美，但第一次训练模型能取得这个成果已经相当不错。况且这还是在我们自己电脑上跑起来的。

我们把这套训练过程称为预训练（pretrain）。之所以叫“预”训练，意味着这只是训练的第一个阶段。虽然本文提供的例子只用了古诗，但对于实际的大模型来说，文本要足够多，种类尽量丰富，让模型通过阅读这些文字全面地理解这个世界，才能在给定任意输入的时候预测出下一个字。

然而，预训练之后的模型只会不停地写诗，它不懂得何时停止，也不会听从我们的指令。下一节，我们将了解如何让模型听从我们的指挥，真正成为人类的工具。
