# 2.9 预训练

细心的话，你或许会发现图2.7的输入和输出维度完全一致，这是精心设计的结果。transformer就像一块积木，同样规格的输入输出使得人们可以把它一层一层搭在一起，从而提升整个模型的能力。

在AI领域有一个神奇的规模定律（Scaling Law）。越多的参数、算力和数据，就能训练出越强的模型。大模型之所以取得突破，正是因为算力上来了。当模型变大，效果自然展现。

虽然本文使用一个教学性质的小模型，但多层transformer仍然是必须的。我们把图2.7的内容用一个transformer block代替，在下图中展示完整的模型结构和训练过程。

<figure><img src="../.gitbook/assets/tutorial_llm.png" alt=""><figcaption><p>图2.8 TutorialLLM的网络结构</p></figcaption></figure>

输入数据是2.3节展示过的一段诗句——“白日依山盡，黃河入海”，对应的预期输出是“流”，图中直接用ID表示文字。输入的ID先经过一个Embedding Table，查表找出每个ID对应的64维向量。注意，前面讲解时我们一直说字向量维度是1024。但受计算资源限制，要想在自己电脑上跑起来，只好把维度降低到64。Embedding Table的输出是10个64维向量，对应输入的10个字。

这组向量经过4个Transformer Block，输出仍然是10个64维向量。中间的过程想必读者已经了解，仍不清楚的同学可以回到前几节温习一下。

接下来，我们用一个Unembedding Matrix将每个64维向量变成8548维向量，这是Embedding Table的逆过程。在Embedding Table里面，我们直接通过查表就能拿到每个ID对应的向量。然而实际上，查表操作可以等价于一次矩阵乘法。举例来说，对于第一个ID 4403，要想取出Embedding Table的第4403列向量，可以通过下面的矩阵运算来实现。

$$
\begin{bmatrix}
c_0 & c_1 & \dots & c_{8547}
\end{bmatrix}
 \begin{bmatrix}
0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 0
\end{bmatrix}
\rightarrow c_{4403}
$$

其中，ci表示Embedding Table的第i个列向量。我们只需要构造一个8548维的One-Hot向量，它的第4403个数是1，其余都是0。这样运算的结果就恰好是Embedding Table的第4403个列向量。

反过来，我们的Unembedding Table可以当作Embedding Table的逆矩阵，用它左乘64维向量，就可以得到一个8548维向量。但此刻得到的8548维向量显然不会恰好One-Hot，只不过，我们训练的目的是让它尽量接近One-Hot。

经过Unembedding Table之后，我们使用最后一个字的8548维向量与期望的输出ID求交叉熵损失（Cross Entropy Loss）。这个损失函数会衡量实际的概率分布与预期概率分布之间的差距。如果把One-Hot向量看作某个位置概率为1，其它位置概率为0。Unembedding Table的输出就可以当作每个位置各自的概率。交叉熵损失正好用来计算它们之间的概率差距。

以上，我们看到了一条训练数据从输入到输出的全过程。交叉熵损失的结果会被用来计算梯度，并反向传播通知模型的每个参数。在整个神经网络中，每个可训练参数都需要对最后的误差负责。根据整个网络的连接关系和各个参数的权重，反向传播算法可以计算出各个参数在其中起到的作用，从而微调它们的值。

