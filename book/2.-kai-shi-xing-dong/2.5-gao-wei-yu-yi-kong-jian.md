# 2.5 高维语义空间

先来尝试解决输入无意义的问题。

回想我们自己，当读到一个字或词的时候，几乎瞬间就能意识到这个词的含义。这个过程甚至是下意识的，仿佛大脑中隐含着一个字典，一看到这个字就能立即跳转到其解释。但此字典非我们前面提到的字典。在2.3节，我们只是把字转换到了ID，而大脑则会更进一步，把字转换成它的含义。我们尚不知道这种含义如何在大脑中存储和表示，但我们可以明确的知道，它一定不只是一个ID。

从数学的角度上看，任何事物都可以用数字表示。ID可以用数字表示，事物的含义其实也可以用数字表示。不过，当我们用数字表示含义的时候，它必须具有内在价值，或者说存在物理解释。比如，用摄氏度表示温度的时候，每个数字都与具体的物理世界相关联，0℃正好对应着冰的熔点。这种物理解释将数字本身的性质与物理现象统一起来，越小的数代表越低的温度，而且温度变化和数字变化的规律是一致的，也就是说，数字加减乘除的结果仍然可以解释，于是对温度的研究就可以转化为数学运算。

回到文字上，文字在大脑中的理解其实也存在物理解释，它可能是神经元突触间特定的激活模式，只是我们不十分了解。既然如此，不如由我们自己来创造一个物理解释。

首先，文字的含义本质上是信息。信息是一个很广的概念，任何事物，实在的或抽象的，都可以认为是信息。信息产生的目的是方便人们沟通，当我说“我很渴”的时候，你就能想到我需要喝水。“渴”和“水”这两个字虽然一个抽象一个具体，指向不同的事物，但它们之间有着紧密的联系。本质上，这种关联衡量了事物之间的相似性。而整个世界恰恰是由相似性联结起来的，你可以试着想象，如果任何事物都和其它事情毫无关联，我们该如何理解这个世界。

那么，以相似性为基础，我们应该让相似的字用相似的数来表示。就像温度一样，相近的温度对应的数值应该更接近。但对文字来说这个要求很难满足，因为字和字之间的相似性不是单调排布的。“渴”和“水”含义相近，这种相似体现在动物的生理行为层面。同时，我也可以说“水”和“油”相似，因为它们都是液体。但由此并不能说明“渴”和“油”相似，因为渴了并不应该喝油。于是，这种相似性的排序会成为巨大的难题。我们不可能在一条数轴上安排好所有字的合适位置，换句话说，这意味着我们不能用一个数来代表一个字。

说到这里，读者或许早就明白了我的意图——我们应该用多个数来表示一个字。如果用两个数表示一个字，就意味着把所有字安排在二维平面上。如果用三个数，就意味着把所有字安排在三维空间中。然而，即使是三维空间也仍然不够用，它不足以区分文字内在的微妙含义。因为本质上，几维空间就意味着我们需要在几个层面上把所有字排好序。可想而知，任何一个字恐怕都包含了不止三重含义，所以三维是不够的。

继续升维，就进入了所谓的高维空间。我们不如一步到位，选一个很高的维度，以免不够用，假设是1024维。用1024个数来表示一个字，每个数代表某个特定层面的微妙含义，我估计是够了。

不过，我们如何定义每个维度的含义呢？很显然，我们定义不了，根本不知道应该怎么找出1024种微妙的解释。但没关系，别忘了我们有神经网络。上一节，我们事先定义好输入的维度和每个维度的解释，然后交给神经网络处理。实际上，我们也可以不定义每个维度的解释，只是告诉神经网络输入维度是1024，让它通过数据自己学习每个维度的意义。

但没有解释的话，输入的这1024个值如何确定呢？

既然我们已经决定让模型自己学习每个维度的解释，那索性每个维度的值也交给模型学习就好了。一开始，我们只需要设置1024个随机值，模型在训练的过程中自然会明白如何合理地拆分字的含义，将其融入这1024个维度中。

为了更方便理解，我们把2.3节的输入数据拿过来试试。首先，输入输出的文本长这样

> **Input**: 白日依山盡，黃河入海
>
> **Output**: 流

紧接着，它们被转换为ID。

> **Input**: \[4403, 2704, 345, 1642, 4450, 8347, 8252, 3407, 536, 3503]
>
> **Outpu**: 3486

然后，就来到了本节介绍的操作，把每个ID转换为一个1024维的向量。大概长这样

> **Input**: \[
>
> \[2.4889e-02, 1.0818e+00, 1.0172e+00, -2.5224e+00, ...(省略1000个数)],
>
> \[1.2428e-01, -2.5663e-01, -9.8576e-02, -2.9525e-02, ...(省略1000个数)],
>
> ...(省略8行)
>
> ]
>
> **Output**: \[-4.7831e-01, 4.5034e-01, -1.5498e+00, -1.3663e+00, ...(省略1000个数)]

虽然说是随机初始化，但其实你会发现，每个数都在0附近不大的范围内。事实上，这些随机数是以0为均值，以1为方差采样得到的。这样做可以保证输入有一个合理的区间，不至于太大或太小。

不过，读者可能仍然会感到迷惑，输入都是随机数，输出也是随机数，那模型到底在学什么呢？

如果你想到了这个问题，说明你关注到了重点。这里的随机1024维向量并不是在每次输入时生成的，而是在创建字典时生成的。也就是说，每个向量唯一对应于字典中的一个ID。虽然训练的时候模型会调整向量的值，但特定的向量始终和特定的字绑定，正是这种绑定让我们的训练变得有意义。
