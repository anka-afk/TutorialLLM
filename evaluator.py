import math
import torch

from dataset import Dataset
from model import TutorialLLM

class Evaluator():

    def __init__(self, model: TutorialLLM, dataset: Dataset, device: str, iterations_to_evaluate_pretrain: int, interval_to_evaluate_pretrain: int, interval_to_evaluate_finetune: int, interval_to_evaluate_alignment: int) -> None:
        self.model = model
        self.dataset = dataset
        self.device = device
        self.iterations_to_evaluate_pretrain = iterations_to_evaluate_pretrain
        self.interval_to_evaluate_pretrain = interval_to_evaluate_pretrain
        self.interval_to_evaluate_finetune = interval_to_evaluate_finetune
        self.interval_to_evaluate_alignment = interval_to_evaluate_alignment

        self.train_loss_sum = math.nan
        self.test_input = '<INS>請用以下題目寫一首詩<INP>月色<RES>'
    
    def reset(self):
        self.train_loss_sum = math.nan

    @torch.no_grad()
    def evaluate_pretrain(self, iteration: int, train_loss: float):
        if iteration % self.interval_to_evaluate_pretrain == 0:
            # Calculate the average loss for this interval
            mean_loss_train = self.train_loss_sum / self.interval_to_evaluate_pretrain
            self.train_loss_sum = 0
            evaluate_loss = self.evaluate_pretrain_loss(self.iterations_to_evaluate_pretrain)
            print(f"Step {iteration}, train loss {mean_loss_train:.4f}, evaluate loss {evaluate_loss:.4f}")

            # Let's generate a poem starting with the word '月' to see how the model is doing
            test_tokens = torch.tensor(self.dataset.encode('月'), dtype=torch.long, device=self.device).unsqueeze(0)
            print('Generate first 100 characters of poems starting with 月:')
            print(self.dataset.decode(self.model.generate(test_tokens, max_new_tokens=100)[0].tolist()))
        
        # Accumulate the training loss
        self.train_loss_sum += train_loss.item()
    
    @torch.no_grad()
    def evaluate_pretrain_loss(self, iterations: int = 100):
        losses = torch.zeros(iterations)
        # Evaluate the model `iterations` times
        for k in range(iterations):
            # Get a batch of pretrain data and compute the loss
            token_ids, labels = self.dataset.get_batch_pretrain('evaluate')
            logits, loss = self.model(token_ids, labels)
            losses[k] = loss.item()
        loss = losses.mean()
        return loss
    
    @torch.no_grad()
    def evaluate_finetune(self, epoch: int, iteration: int, train_loss: float):
        if iteration % self.interval_to_evaluate_finetune == 0:
            # Calculate the average loss for this interval
            mean_loss_train = self.train_loss_sum / self.interval_to_evaluate_finetune
            self.train_loss_sum = 0
            evaluate_loss = self.evaluate_finetune_loss()
            print(f"Epoch {epoch}, step {iteration}, train loss {mean_loss_train:.4f}, evaluate loss {evaluate_loss:.4f}")

            # Let's generate a poem with a given title to see how the model is doing
            test_tokens = torch.tensor(self.dataset.encode(self.test_input), dtype=torch.long, device=self.device).unsqueeze(0)
            output = self.dataset.decode(self.model.generate(test_tokens, max_new_tokens=100)[0].tolist())
            # Truncate the output to the '\0' character
            output = output[:output.find('\0')]
            print('Generate a complete poem for title 月色:')
            print(output[len(self.test_input):])
        
        # Accumulate the training loss
        self.train_loss_sum += train_loss.item()

    @torch.no_grad()
    def evaluate_finetune_loss(self):
        loss_sum = 0
        # Get a batch generator of finetune data
        batch_generator = self.dataset.get_batch_generator_finetune('evaluate')
        # Evaluate the model by processing all batches generated by the generator
        for k, batch in enumerate(batch_generator):
            token_ids, labels = batch
            logits, loss = self.model(token_ids, labels)
            loss_sum += loss.item()
        loss = loss_sum / (k + 1)
        return loss